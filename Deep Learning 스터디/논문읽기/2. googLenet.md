## Inception 구조
- Kernel 크기를 1X1, 3X3, 5X5 연산을 통해 다양한 Scale로 Feature가 추출 된다.
![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbKbnNV%2FbtqSxB8J5jA%2F2CjFDvUTU9wIKJ1NSNBWy0%2Fimg.png)
- 또한, 다음 레이어는 서로 다른 Layer에서 Feature를 추출 할 수 있다.
- ![image](https://blog.kakaocdn.net/dn/by996R/btqSAR4zKGf/TkPxN652gdxzEJu1YtzWh1/img.png)
- 이때 문제는 네트워크의 깊이가 깊어 짐에 따라 연산량이 매우 증가 하게 된다
- 이를 해결 하기 위해 3X3 5X5 앞에 1X1 커널을 두어 차원을 줄 인다.

## global average pooling
- FC 방식 대신 global average pooling 방식을 사용한다.
- gloabl average pooling은 전 층에서 산출된 특성맵들을 각각 평균낸 것을 1차원 벡터로 만들어 주는 것이다.(1차원 벡터로 만들어야 softmax 층을 연결 해줄수 있기 때문이다.)
- 이를 통해 가중치의 개수를 많이 없애 준다.
- ![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbwTHh0%2FbtqB2uyArWI%2FqBr48Ik8bl4bK1oOEJa3bk%2Fimg.png)

## auxiliary classifier
- 네트워크가 깊어질 수록 vgradient vanishing 문제가 발생한다.
- 이를 해결하기 위해 googlenet은 네트워크 중간에 두개의 보조 분류기(auxiliary classifier)를 사용한다.
- 중간중간에 결과를 출력해 추가적인 역전파를 일으켜 gradient가 끝까지 전달 될 수 있게 한다.
- 지나치게 영향을 주는 것을 방지 하기 위해 auxiliary classifier loss에 0.3을 곱한다.