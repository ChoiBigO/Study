## Abstract
- Residual Learning이란 이전 Layer의 결과를 다시 이용하는 것이다.
- 입력 Layer를 다시 이용하여 더 쉬운 최적화와 깊은 네트워크에서의 정확도 향상이 가능 했다.

## Intorduction
- 신경망은 추상화에 대해 low / mid / high level의 특징을 multi-layer 방식으로 통합한다.
- 각 추상화 level은 쌓인 layer의 수에 따라 높아 진다.
- 그러나 Layer가 많아 질수록  Degradation : (**Vanishing** / **Exploding gradient**) 현상이 생긴다.

#### Vanishing Gradient Problem(기울기 소멸 문제)
- Back Propagation에서 계산 결과와 정답과의 오차를 통해 가중치를 수정하는데, 입력층으로 갈수록 기울기가 작아져 가중치들이 업데이트 되지 않아 최적의 모델을 찾을 수 없는 문제이다.
- 은닉층을 많이 거칠 수록 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상이 발생한다.
- 기울기가 0으로 소멸되어 버리면 네트워크의 학습은 매우 느려지고, 학습이 다 이루어지지 않는 상태에서 멈추게 된다.
- ReLU, Leaky ReLU 활성화 함수를 사용하거나 Batch Normalization 기법을 사용해 이를 해결한다.

#### Batch Normalization
- 배치 단위로 입력에 대해 평균을 0으로 만들고, 정규화를 한다.
- ???

#### Exploding Gradient 
- 기울기가 점자 커져 가중치들이 비정상적으로 큰 값이 되면서 결국 발산 하기도 한다.

- 기존 네트워크는 입력 x를 받고 layer를 거쳐 H(x)를 출력하는데, 이는 입력값 x를 타겟값 y로 mapping하는 함수 H(x)를 얻는 것이 목적이다.
- Residual Learning은 H(x)가 아닌 출력과 입력의 차인 H(x)-x 를 얻도록 목표를 수정한다.
- Residual Function인 F(x) = H(x) - x를 최소화 시켜야 한다. 즉, 출력과 입력의 차를 줄인다.
- x는 입력 이기 때문에 바꾸지 못한다. F(x)가 0이 되는 것이 최적의 해이고, 결국 H(x) = x가 된다.

## Residual Block
- Residual Block을 이용해 네트워크 최적화 난이도를 낮춘다.
- Input값을 Convolution을 거친 값과 더해준다. H(x) = F(X) + x
- ResNet은 F(X) + x 를 최소화 하는 것을 목적으로 한다, x는 입력이므로 변할수 없기 때문에 F(x)를 0에 가깝게 만드는 것이 목적이 된다.
- F(x)를 최소로 한다는 것은 H(X)+x 를 최소로 하는 것이고, 이떄 H(x) - x 를 잔차(Residual) 이라한다.
- 기존 학습 했던 정보를 그대로 가져오고 추가적으로 학습할 정보만 추가적으로 더한다.
- 또한 H(x)를 미분해도 F(x)' +1 이므로 Gradient Vanishing 현상을 해결 했다.

## ResNet 구조
- 처음을 제외하고 모두 3X3 사이즈의 컴볼루션 필터를 사용했다.
- 맵이 반으로 줄 떄, 맴의 뎁스를 2배로 높였다.

## ResNet 장점
- Shortcut Connection 방식을 이용하면 떨어진 두개의 층을 연결 함으로써 오차 역전파 시 기울기가 쉽게 전파 된다는 장점이 있다.

## Residual Connection with Bottle Neck Layer
- (https://coding-yoon.tistory.com/116?category=825914)
- ResNet은 연산량을 줄이기 위해 Bottle Neck Later를 사용한 Residual Block을 사용한다.
- Input Dimension을 축소하고 마지막에 다시 Dimension을 키운다.